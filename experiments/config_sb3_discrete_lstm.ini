# SB3 Discrete PPO Configuration

[General]
save_dir = ./outputs/20_layer/sb3_discrete_lstm_interleaved
materials_path = ../examples/materials.json

[sb3_discrete_lstm]
total_timesteps = 5000000
n_thickness_bins = 3
verbose = 1
target_reflectivity = 0.99
target_absorption = 1.0
mask_consecutive_materials = True
mask_air_until_min_layers = True
min_layers_before_air = 2
epochs_per_step = 4000
steps_per_objective = 10
tensorboard_log = /Users/joebayley/projects/coating_optimisation/sb3_refactor_outputs/sb3_discrete/logs

# LSTM-specific parameters
lstm_hidden_size = 128
lstm_num_layers = 2
lstm_features_dim = 128


[Data]
# Layer configuration
n_layers = 20
min_thickness = 0.1
max_thickness = 0.4
use_optical_thickness = True

# Optimization objectives
optimise_parameters = ["reflectivity", "absorption"]
optimise_targets = {"reflectivity": 0.9999999, "absorption": 0.0}
objective_bounds = {"reflectivity": [0.0, 0.99999], "absorption": [400000, 0]}

# Reward settings
combine = sum
use_reward_normalisation = True
reward_normalisation_apply_clipping = True
apply_air_penalty = True
air_penalty_weight = 0.5

# Other settings
ignore_air_option = False
ignore_substrate_option = False
use_intermediate_reward = False
apply_preference_constraints = True

constraint_schedule = "interleaved"

[Training]
cycle_weights = random
