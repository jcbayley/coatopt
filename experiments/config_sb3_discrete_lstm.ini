# SB3 Discrete PPO Configuration

[General]
save_dir = ./outputs/20_layer/sb3_discrete_lstm_interleaved_2
materials_path = ../examples/materials.json

[sb3_discrete_lstm]
total_timesteps = 5000000
n_thickness_bins = 3
verbose = 1
mask_consecutive_materials = True
mask_air_until_min_layers = True
min_layers_before_air = 1
epochs_per_step = 400
steps_per_objective = 12

# Constraint settings
constraint_penalty = 10.0

# Entropy annealing
max_entropy = 0.2
min_entropy = 0.01
adaptive_entropy_to_constraints = True

# LSTM-specific parameters
lstm_hidden_size = 32
lstm_num_layers = 4
lstm_features_dim = 32

pareto_dominance_bonus = 2.0


[Data]
# Layer configuration
n_layers = 20
min_thickness = 0.25
max_thickness = 0.25
use_optical_thickness = True

# Optimization objectives
optimise_parameters = ["reflectivity", "absorption"]
optimise_targets = {"reflectivity": 0.9999999, "absorption": 0.0}
objective_bounds = {"reflectivity": [0.0, 0.999999], "absorption": [50000, 1e-3]}

# Reward settings
combine = sum
use_reward_normalisation = True
reward_normalisation_apply_clipping = True
apply_air_penalty = True
air_penalty_weight = 0.5

# Other settings
ignore_air_option = False
ignore_substrate_option = False
use_intermediate_reward = False
apply_preference_constraints = True

constraint_schedule = "interleaved"

[Training]
cycle_weights = random

[Algorithm]
# PPO hyperparameters
learning_rate = 3e-4
n_steps = 64
batch_size = 8
n_epochs = 3
gamma = 0.99
gae_lambda = 0.95
clip_range = 0.2
ent_coef = 0.0
vf_coef = 0.5
max_grad_norm = 0.5

# Network architecture (for LSTM, smaller networks after feature extraction)
net_arch_pi = [64, 64]
net_arch_vf = [64, 64]
