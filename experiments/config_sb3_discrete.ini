[general]
# base dir
save_dir = ./runs
# materials file
materials_path = ./materials.json
# Optional: append custom name to run directory
run_name = ppo_5bin_expreward_hyperv6_minnetconstraints_run1
# Disable MLflow logging (default: True)
disable_mlflow = True
mlflow_log_freq=200

[sweep]
# Hyperparameter sweep configuration
metric = hypervolume
direction = maximize

# Active sweep parameters
n_startup_trials = 5
param_sb3_discrete.epochs_per_step = {'type': 'int', 'min': 500, 'max': 2000}
param_algorithm.ent_coef = {'type': 'float', 'min': 0.01, 'max': 0.1}

[sb3_discrete]
# total training steps (this is epochs)
total_timesteps = 600000
# how many discrete bins for thickness
n_thickness_bins = 5
# print extra information
verbose = 1
# make sure not same material used consecutively
mask_consecutive_materials = True
# set a minimum number of layers before air can be used
mask_air_until_min_layers = True
min_layers_before_air = 4

# constraints
# how many epochs to use before iterating constraint
epochs_per_step = 1000
# how many constraint steps per objective
steps_per_objective = 20

# Constraint settings
constraint_penalty = 10.0
# extra bonus for improved hypervolume of pareto front
pareto_dominance_bonus = 50

# Entropy annealing
max_entropy = 0.01
min_entropy = 0.01
adaptive_entropy_to_constraints = False

# Policy reset
reset_policy_each_phase = True


[data]
# Layer configuration
n_layers = 20
min_thickness = 0.1
max_thickness = 0.4
use_optical_thickness = True

# Optimization objectives
optimise_parameters = ["reflectivity", "absorption"]
optimise_targets = {"reflectivity": 1.0, "absorption": 0.0}
objective_bounds = {"reflectivity": [0.0, 0.999999], "absorption": [50000, 1e-3]}

# Reward settings
combine = sum
use_reward_normalisation = True
reward_normalisation_apply_clipping = False
apply_air_penalty = True
air_penalty_weight = 0.5

# Other settings
ignore_air_option = False
ignore_substrate_option = False
use_intermediate_reward = False
apply_preference_constraints = True
constraint_schedule = "interleaved"

[training]
cycle_weights = random

[algorithm]
# PPO hyperparameters
learning_rate = 5e-4
n_steps = 128
batch_size = 32
n_epochs = 10
gamma = 0.99
gae_lambda = 0.95
clip_range = 0.2
ent_coef = 0.01
vf_coef = 0.5
max_grad_norm = 0.5

# Network architecture
pre_network = mlp
net_arch_pi = [128, 64, 32]
net_arch_vf = [128, 64, 32]
